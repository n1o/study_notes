# Activation functions

In every [neural network](neural_networks.md) we have an activation function.

$$
g(Wx + b)
$$

## [RELu](rectified_hidden_unit.md) (Rectified linear activation function)
Similar to linear, because of this it is easy to optimize.